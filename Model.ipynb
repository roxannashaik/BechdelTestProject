{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "import string\n",
    "from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, auc\n",
    "from sklearn.metrics import roc_curve, make_scorer, precision_score, recall_score, f1_score\n",
    "from sklearn import tree\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "from matplotlib.legend_handler import HandlerLine2D \n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from vecstack import stacking\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(['ourselves', 'between', 'but', 'again','there', 'about', 'once', \\\n",
    "                 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an',\\\n",
    "                 'be', 'some', 'for', 'do', 'its', 'such', 'into', 'of', 'most', \n",
    "                 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from',\\\n",
    "                 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your',\\\n",
    "                 'through', 'don', 'nor', 'me', 'were', 'more',\\\n",
    "                 'this', 'down', 'should', 'our', 'their', 'while', 'above', \\\n",
    "                 'both', 'up', 'to', 'ours', 'had', 'all', 'no', 'when'\\\n",
    "                 , 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', \\\n",
    "                 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', \\\n",
    "                 'what', 'over', 'why', 'so', 'can', 'did', 'not', 'now', 'under', \\\n",
    "                 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those'\\\n",
    "                 , 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against',\\\n",
    "                 'a', 'by', 'doing', 'it', 'how','b','the', 'you', 'further', 'href', \\\n",
    "                 'was', 'here', 'than','you'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine('postgresql://postgres:stat170@postgres/stat170a', client_encoding='utf8', pool_pre_ping=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bech = pd.read_sql_query('SELECT * from bechdal_test;',con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scripts1 = pd.read_sql_query('SELECT * from scripts_final;',con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1_t = scripts1['name']\n",
    "s1_t = s1_t.replace(\"[\\s]+\", \" \", regex=True).str.strip()\n",
    "scripts1['name'] = s1_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.merge(scripts1,bech, left_on = 'name', right_on='title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d[d['convo'].apply(lambda x: len(x.split(' ')) > 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+')\n",
    "stemmer = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    data = data.strip().lower()\n",
    "    return data.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(data):\n",
    "    return stemmer.stem(tokenizer.tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d['convo'] = d['convo'].apply(clean)\n",
    "d['info'] = d['info'].apply(clean)\n",
    "d['bechdal_int'] = d['bechdal'].astype(int)\n",
    "d['text_info'] = d[['convo', 'info']].apply(lambda x: ''.join(x), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d.to_cdv(\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer={'accuracy': make_scorer(accuracy_score),\n",
    "        'f1_score': make_scorer(f1_score),\n",
    "        'precision': make_scorer(precision_score, pos_label=1, average='binary'),\n",
    "        'recall': make_scorer(recall_score), \n",
    "        'mean_squared_error': make_scorer(mean_squared_error)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test= train_test_split(d.loc[:, 'text_info'],\\\n",
    "                                                       d.loc[:, 'bechdal_int'], test_size = 0.2)\n",
    "v2 = TfidfVectorizer(analyzer='word',use_idf=True, ngram_range=(2,2), \\\n",
    "                         token_pattern= r'\\w{1,}', stop_words=stopwords, sublinear_tf=True)\n",
    "train_X = v2.fit_transform(X_train)\n",
    "test_X = v2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators' : [10, 20, 50,100,150,200]}\n",
    "rf_clf = RandomForestClassifier(max_depth=4,\n",
    "                               random_state=0)\n",
    "rf_gs_clf = GridSearchCV(rf_clf, parameters, cv=5,\\\n",
    "                      return_train_score=True, \\\n",
    "                      scoring=scorer,refit='mean_squared_error' )\n",
    "rf_gs_clf.fit(train_X, y_train)\n",
    "rf_gs_y_pred = rf_gs_clf.predict(test_X)\n",
    "rf_grid_search_results = rf_gs_clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search_results_with_scorer = rf_gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs_test = [x for x in rf_grid_search_results_with_scorer['mean_test_mean_squared_error']]\n",
    "rf_gs_tr = [x for x in rf_grid_search_results_with_scorer['mean_train_mean_squared_error']]\n",
    "line1, = plt.plot(rf_grid_search_results_with_scorer['param_n_estimators'].data, rf_gs_tr, 'purple', label=\"Train MSE\")\n",
    "line2, = plt.plot(rf_grid_search_results_with_scorer['param_n_estimators'].data, rf_gs_test, 'pink', label=\"Test MSE\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('N_estimators of Random Forest')\n",
    "plt.title(\"Random Forest with GridSearch: MSE for different N estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_gs_test_acc = [x for x in rf_grid_search_results_with_scorer['mean_test_accuracy']]\n",
    "rf_gs_tr_acc = [x for x in rf_grid_search_results_with_scorer['mean_train_accuracy']]\n",
    "line1, = plt.plot(rf_grid_search_results_with_scorer['param_n_estimators'].data, rf_gs_tr_acc, 'purple', label=\"Train MSE\")\n",
    "line2, = plt.plot(rf_grid_search_results_with_scorer['param_n_estimators'].data, rf_gs_test_acc, 'pink', label=\"Test MSE\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Accuracy of Random Forest')\n",
    "plt.title(\"Random Forest with GridSearch: Accuracy for different N estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#D Tree Depth 2 was best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'max_depth' : range(2,10)}\n",
    "dt_clf = tree.DecisionTreeClassifier()\n",
    "dt_gs_clf = GridSearchCV(dt_clf, parameters, cv=5,\\\n",
    "                      return_train_score=True, \\\n",
    "                      scoring=scorer,refit='mean_squared_error' )\n",
    "dt_gs_clf.fit(train_X, y_train)\n",
    "dt_gs_y_pred = dt_gs_clf.predict(test_X)\n",
    "dt_grid_search_results = dt_gs_clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_grid_search_results_with_scorer = dt_gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gs_test = [x for x in dt_grid_search_results_with_scorer['mean_test_mean_squared_error']]\n",
    "dt_gs_tr = [x for x in dt_grid_search_results_with_scorer['mean_train_mean_squared_error']]\n",
    "line1, = plt.plot(dt_grid_search_results_with_scorer['param_max_depth'].data, dt_gs_tr, 'purple', label=\"Train MSE\")\n",
    "line2, = plt.plot(dt_grid_search_results_with_scorer['param_max_depth'].data, dt_gs_test, 'pink', label=\"Test MSE\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Depth of Tree')\n",
    "plt.title(\"Descision Tree with GridSearch: MSE for different Depth Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_gs_test_acc = [x for x in dt_grid_search_results_with_scorer['mean_test_accuracy']]\n",
    "dt_gs_tr_acc = [x for x in dt_grid_search_results_with_scorer['mean_train_accuracy']]\n",
    "line1, = plt.plot(dt_grid_search_results_with_scorer['param_max_depth'].data, dt_gs_tr_acc, 'purple', label=\"Train Accuracy\")\n",
    "line2, = plt.plot(dt_grid_search_results_with_scorer['param_max_depth'].data, dt_gs_test_acc, 'pink', label=\"Test Accuracy\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Depth of Tree')\n",
    "plt.title(\"Descision Tree with GridSearch: Accuracy for different Depth Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dt_gs_test = [x for x in grid_search_results_with_scorer['mean_test_mean_squared_error']]\n",
    "dt_gs_tr = [x for x in grid_search_results_with_scorer['mean_train_mean_squared_error']]\n",
    "line1, = plt.plot(grid_search_results_with_scorer['param_max_depth'].data, dt_gs_tr, 'purple', label=\"Train MSE\")\n",
    "line2, = plt.plot(grid_search_results_with_scorer['param_max_depth'].data, dt_gs_test, 'pink', label=\"Test MSE\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Depth of Tree')\n",
    "plt.title(\"Descision Tree with GridSearch: MSE for different Depth Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'solver' : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
    "lr_clf =  LogisticRegression()\n",
    "lr_gs_clf = GridSearchCV(lr_clf, parameters, cv=5,\\\n",
    "                      return_train_score=True, \\\n",
    "                      scoring=scorer,refit='mean_squared_error' )\n",
    "lr_gs_clf.fit(train_X, y_train)\n",
    "lr_gs_y_pred = lr_gs_clf.predict(test_X)\n",
    "lr_grid_search_results = lr_gs_clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_grid_search_results_with_scorer = lr_gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs_test = [x for x in lr_grid_search_results_with_scorer['mean_test_mean_squared_error']]\n",
    "lr_gs_tr = [x for x in lr_grid_search_results_with_scorer['mean_train_mean_squared_error']]\n",
    "line1, = plt.plot(lr_grid_search_results_with_scorer['param_solver'].data, lr_gs_tr, 'purple', label=\"Train MSE\")\n",
    "line2, = plt.plot(lr_grid_search_results_with_scorer['param_solver'].data, lr_gs_test, 'pink', label=\"Test MSE\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Solver')\n",
    "plt.title(\"Logistic Regression with GridSearch: MSE for different Solvers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_gs_test_acc = [x for x in lr_grid_search_results_with_scorer['mean_test_accuracy']]\n",
    "lr_gs_tr_acc = [x for x in lr_grid_search_results_with_scorer['mean_train_accuracy']]\n",
    "line1, = plt.plot(lr_grid_search_results_with_scorer['param_solver'].data, lr_gs_tr, 'purple', label=\"Train Accuracy\")\n",
    "line2, = plt.plot(lr_grid_search_results_with_scorer['param_solver'].data, lr_gs_test, 'pink', label=\"Test Accuracy\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Solver')\n",
    "plt.title(\"Logistic Regression with GridSearch: Accuracy for different Solvers\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'alpha' : [0.001,0.01,0.1,.4,.5,.75,1,2]}\n",
    "nb_clf =  MultinomialNB()\n",
    "nb_gs_clf = GridSearchCV(nb_clf, parameters, cv=5,\\\n",
    "                      return_train_score=True, \\\n",
    "                      scoring=scorer,refit='mean_squared_error' )\n",
    "nb_gs_clf.fit(train_X, y_train)\n",
    "nb_gs_y_pred = nb_gs_clf.predict(test_X)\n",
    "nb_grid_search_results = nb_gs_clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_grid_search_results_with_scorer = nb_gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gs_test = [x for x in nb_grid_search_results_with_scorer['mean_test_mean_squared_error']]\n",
    "nb_gs_tr = [x for x in nb_grid_search_results_with_scorer['mean_train_mean_squared_error']]\n",
    "line1, = plt.plot(nb_grid_search_results_with_scorer['param_alpha'].data, nb_gs_tr, 'purple', label=\"Train MSE\")\n",
    "line2, = plt.plot(nb_grid_search_results_with_scorer['param_alpha'].data, nb_gs_test, 'pink', label=\"Test MSE\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('Alpha')\n",
    "plt.title(\"Naive Bayes with GridSearch: MSE for different Alphas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_gs_test_acc = [x for x in nb_grid_search_results_with_scorer['mean_test_accuracy']]\n",
    "nb_gs_tr_acc = [x for x in nb_grid_search_results_with_scorer['mean_train_accuracy']]\n",
    "line1, = plt.plot(nb_grid_search_results_with_scorer['param_alpha'].data, nb_gs_tr, 'purple', label=\"Train Accuray\")\n",
    "line2, = plt.plot(nb_grid_search_results_with_scorer['param_alpha'].data, nb_gs_test, 'pink', label=\"Test Accuray\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('Accuray')\n",
    "plt.xlabel('Alpha')\n",
    "plt.title(\"Naive Bayes with GridSearch: Accuray for different Alphas\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ada boost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_dt_clf = tree.DecisionTreeClassifier(max_depth=2)\n",
    "parameters = { 'n_estimators': [10,20,30,40,50,60,70]}\n",
    "ab_clf =  AdaBoostClassifier(ad_dt_clf)\n",
    "ab_gs_clf = GridSearchCV(ab_clf, parameters, cv=5,\\\n",
    "                      return_train_score=True, \\\n",
    "                      scoring=scorer,refit='mean_squared_error' )\n",
    "ab_gs_clf.fit(train_X, y_train)\n",
    "ab_gs_y_pred = ab_gs_clf.predict(test_X)\n",
    "ab_grid_search_results = ab_gs_clf.cv_results_.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_grid_search_results_with_scorer = ab_gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_gs_test = [x for x in ab_grid_search_results_with_scorer['mean_test_mean_squared_error']]\n",
    "ab_gs_tr = [x for x in ab_grid_search_results_with_scorer['mean_train_mean_squared_error']]\n",
    "ab_gs_lab = ab_grid_search_results_with_scorer['param_n_estimators'].data\n",
    "line1, = plt.plot(ab_gs_lab , ab_gs_tr, 'purple', label=\"Train MSE\")\n",
    "line2, = plt.plot(ab_gs_lab, ab_gs_test, 'pink', label=\"Test MSE\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('MSE')\n",
    "plt.xlabel('n estimators')\n",
    "plt.title(\"AdaBoost with GridSearch: MSE for different N estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_gs_test_acc = [x for x in ab_grid_search_results_with_scorer['mean_test_accuracy']]\n",
    "ab_gs_tr_acc = [x for x in ab_grid_search_results_with_scorer['mean_train_accuracy']]\n",
    "ab_gs_lab = ab_grid_search_results_with_scorer['param_n_estimators'].data\n",
    "line1, = plt.plot(ab_gs_lab , ab_gs_tr_acc, 'purple', label=\"Train Accuracy\")\n",
    "line2, = plt.plot(ab_gs_lab, ab_gs_test_acc, 'pink', label=\"Test Accuracy\")\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "plt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\n",
    "plt.rcParams.update({'font.size': 26})\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('n estimators')\n",
    "plt.title(\"AdaBoost with GridSearch: Accuracy for different N estimators\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dTree = tree.DecisionTreeClassifier(max_depth = 4)\n",
    "nb = MultinomialNB(alpha =0.1)\n",
    "rf = RandomForestClassifier(max_depth=4, random_state=0, n_estimators = 50)\n",
    "ab = AdaBoostClassifier(dTree, n_estimators =50)\n",
    "lr = LogisticRegression(solver = 'lbfgs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dn = [dTree,nb]\n",
    "train2, test2 = stacking(dn,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_model = dTree\n",
    "simp_model = simp_model.fit(train2, y_train)\n",
    "y_pred_simp2 = simp_model.predict(test2)\n",
    "accuracy_score(y_test, y_pred_simp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr =[dTree, rf]\n",
    "r1_train, r1_test = stacking(dr,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simp_model = dTree\n",
    "simp_model = simp_model.fit(r1_train, y_train)\n",
    "y_pred_simp2 = simp_model.predict(r1_test)\n",
    "accuracy_score(y_test, y_pred_simp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da =[dTree, ab]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train, r2_test = stacking(da,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = AdaBoostClassifier(dTree, n_estimators = 50)\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl  = [dTree,lr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3_train, r3_test = stacking(dl,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = lr\n",
    "    \n",
    "model3 = model3.fit(r3_train, y_train)\n",
    "y_pred3 = model3.predict(r3_test)\n",
    "accuracy_score(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr = [nb, rf]\n",
    "r2_train, r2_test = stacking(nr,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nb\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = [nb, ab]\n",
    "r2_train, r2_test = stacking(na,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nb\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "na = [nb, ab]\n",
    "r2_train, r2_test = stacking(na,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = nb\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nl = [nb, lr]\n",
    "r2_train, r2_test = stacking(nl,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lr\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra = [rf, ab]\n",
    "r2_train, r2_test = stacking(ra,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = ab\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl = [rf, lr]\n",
    "r2_train, r2_test = stacking(rl,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lr\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = [ab, lr]\n",
    "r2_train, r2_test = stacking(al,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = lr\n",
    "    \n",
    "model2 = model2.fit(r2_train, y_train)\n",
    "y_pred2 = model2.predict(r2_test)\n",
    "accuracy_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndr =[nb,dTree, rf]\n",
    "r1_train, r1_test = stacking(ndr,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = nb  \n",
    "model3 = model3.fit(r1_train, y_train)\n",
    "y_pred3 = model3.predict(r1_test)\n",
    "accuracy_score(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dln =[nb,dTree, lr]\n",
    "r1_train, r1_test = stacking(dln,train_X, y_train, test_X, regression=False, \\\n",
    "                           mode='oof_pred_bag', save_dir=None, metric=accuracy_score, \\\n",
    "                           n_folds=5, stratified=True,shuffle=True,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = lr\n",
    "model3 = model3.fit(r1_train, y_train)\n",
    "y_pred3 = model3.predict(r1_test)\n",
    "accuracy_score(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
